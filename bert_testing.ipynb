{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jstephencorey/bert-experiments/blob/main/bert_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4gk4mBXNE5d",
        "outputId": "9ab67f1e-7c44-493b-836a-8dca4a3d929e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bert_git'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 50 (delta 13), reused 16 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (50/50), done.\n",
            "['/content', '/env/python', '/usr/lib/python38.zip', '/usr/lib/python3.8', '/usr/lib/python3.8/lib-dynload', '', '/usr/local/lib/python3.8/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.8/dist-packages/IPython/extensions', '/root/.ipython', '/content/bert_git']\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.3/174.3 KB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "on_colab = True\n",
        "first_run = True # Change this when you disconnect and reconnect the runtime\n",
        "if on_colab and first_run:\n",
        "    !git clone https://github.com/jstephencorey/bert-experiments.git  bert_git # Only need to run this on colab\n",
        "    from pathlib import Path\n",
        "    import sys\n",
        "    sys.path.append(str('/content/bert_git')) #Only need on the colab\n",
        "    print(sys.path)\n",
        "    !pip install transformers --quiet\n",
        "    !pip install wandb --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiNtU2K1NE5i",
        "outputId": "747d404d-5923-4e90-a10e-891d16baf833"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import random\n",
        "from torch import optim\n",
        "import bert\n",
        "import wandb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch import autocast\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Tt1jGO5P25I",
        "outputId": "30c68d86-5c28-4824-9a8d-c339b4a18d81"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import the tokenizers"
      ],
      "metadata": {
        "id": "yVJD6juiPgQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "btlU5Is0Pr7N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = '/content/drive/MyDrive/AI_Data/'\n",
        "MODEL_DIR = '/content/drive/MyDrive/AI_Models/storygpt/'\n",
        "TOKENIZER_DIR = f'{MODEL_DIR}storygpt2tokenizer_ft2/'\n",
        "\n",
        "bibliotik_train = f'{DATA_DIR}bibliotik_corpus/biblitik_22500_full.gz'\n",
        "bibliotik_val = f'{DATA_DIR}bibliotik_corpus/biblitik_7500_val.gz'\n",
        "\n",
        "bibliotik_train_tokenized = f'{DATA_DIR}bibliotik_corpus/biblitik_22500_train_256_tokenized.gz'\n",
        "bibliotik_val_tokenized = f'{DATA_DIR}bibliotik_corpus/biblitik_7500_val_256_tokenized.gz'\n",
        "BIBLIOTIK_TRAIN_TOKENIZED_LEN = 2227700\n",
        "BIBLIOTIK_VAL_TOKENIZED_LEN = 741200\n",
        "\n",
        "atlas_shrugged_filename = f'{DATA_DIR}atlas-shrugged.txt'\n",
        "anthem_filename = f'{DATA_DIR}anthem.txt'"
      ],
      "metadata": {
        "id": "A0oATDFxPwsv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BOS_TOKEN = '<BOS>'\n",
        "EOS_TOKEN = '<EOS>'\n",
        "PAD_TOKEN = '<PAD>'\n",
        "CLS_TOKEN = '[CLS]'\n",
        "MASK_TOKEN = '[MASK]'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)\n",
        "\n",
        "special_tokens_dict = {'cls_token':CLS_TOKEN, 'bos_token': BOS_TOKEN, 'eos_token': EOS_TOKEN, 'pad_token': PAD_TOKEN, 'mask_token': MASK_TOKEN}\n",
        "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "VOCAB_SIZE = len(tokenizer)\n",
        "print(VOCAB_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkS_SvWjQV7E",
        "outputId": "2a73d348-7cf6-4a43-dca7-58770bd97112"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "52005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup Wandb config"
      ],
      "metadata": {
        "id": "vIiou_Oty1Wy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "U1Md96FKNE5k"
      },
      "outputs": [],
      "source": [
        "test_parts = True\n",
        "device= \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_number = 0\n",
        "previous_run_numbers = []\n",
        "if run_number in previous_run_numbers:\n",
        "  raise(Exception(\"Pick a new run number!\"))\n",
        "\n",
        "run_name = f\"bert_no_masking_{run_number}\"\n",
        "\n",
        "wandb.init(project=\"bert_experiments\", \n",
        "           name=run_name,\n",
        "           )\n",
        "\n",
        "wandb.config = {\n",
        "  \"seed\":134,\n",
        "  # Dataloader config\n",
        "  \"batch_size\": 2,\n",
        "  \"val_batch_size\": 2,\n",
        "  \"load_chunk_size\": 6,\n",
        "  # Model config\n",
        "  \"d_model\":512,\n",
        "  \"vocab_length\": VOCAB_SIZE,\n",
        "  \"context_len\": 256,\n",
        "  \"num_layers\":6,\n",
        "  \"feed_forward_dimensions\":1024,\n",
        "  \"attention_heads\":8,\n",
        "  \"attention_qkv_dims\":128,\n",
        "  \"dropout\":.1,\n",
        "  \"device\":device,\n",
        "  #Ablation config\n",
        "  \"save_model\": None,\n",
        "  \"bert_mask_percentage\": None,\n",
        "  #Training config\n",
        "  \"lr\": 1e-4,\n",
        "  \"epochs\": 1,\n",
        "  \"batches_per_epoch\":2,\n",
        "  \"batches_per_val\":2,\n",
        "}\n",
        "\n",
        "config = wandb.config\n",
        "\n",
        "random.seed(config['seed'])\n",
        "torch.random.manual_seed(config['seed'])\n",
        "\n",
        "print(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "bwMqSlE8OSgj",
        "outputId": "73f38ab6-6739-43c2-ce0e-9f64a8cc8240"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjstephencorey\u001b[0m (\u001b[33msintez\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230105_012801-2psvw4y7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/sintez/bert_experiments/runs/2psvw4y7\" target=\"_blank\">bert_no_masking_0</a></strong> to <a href=\"https://wandb.ai/sintez/bert_experiments\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'seed': 134, 'batch_size': 2, 'val_batch_size': 2, 'load_chunk_size': 6, 'd_model': 512, 'vocab_length': 52005, 'context_len': 256, 'num_layers': 6, 'feed_forward_dimensions': 1024, 'attention_heads': 8, 'attention_qkv_dims': 128, 'dropout': 0.1, 'device': 'cuda', 'save_model': None, 'bert_mask_percentage': None, 'lr': 0.0001, 'epochs': 1, 'batches_per_epoch': 2, 'batches_per_val': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Get a dataloader"
      ],
      "metadata": {
        "id": "HVjgDbcSV-no"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title BertChunkDataLoader\n",
        "class BertChunkDataLoader():\n",
        " \n",
        "  def __init__(self, df_filename, batch_size, df_file_len, load_chunk_size):\n",
        "    self.df_filename = df_filename\n",
        "    self.batch_size = batch_size\n",
        "    self.total_df_len = df_file_len\n",
        "    self.remaining_df_items = df_file_len\n",
        "    self.load_chunk_size = load_chunk_size\n",
        "    self.df = self.init_df()\n",
        "    self.sequence_buffer = []\n",
        "\n",
        "\n",
        "  def init_df(self):\n",
        "     self.remaining_df_items = self.total_df_len\n",
        "     df = pd.read_csv(self.df_filename, \n",
        "                 index_col=0,\n",
        "                 compression={'method': 'gzip', 'compresslevel': 2}, \n",
        "                 chunksize=self.load_chunk_size, \n",
        "                 iterator=True)\n",
        "     return df\n",
        "\n",
        "  def has_next_batch(self):\n",
        "    return self.remaining_df_items > self.DF_BUFFER\n",
        "\n",
        "  def get_next_seq(self):\n",
        "    if len(self.sequence_buffer) <= 0:\n",
        "      if self.remaining_df_items < self.load_chunk_size:\n",
        "        self.df = self.init_df()\n",
        "      chunk = self.df.get_chunk()\n",
        "      self.remaining_df_items -= self.load_chunk_size\n",
        "      for item in chunk['token_seqs']:\n",
        "        item_arr = self.str_to_arr(item)\n",
        "        self.sequence_buffer.append(item_arr)\n",
        "    return self.sequence_buffer.pop()\n",
        "  \n",
        "  def str_to_arr(self, text):\n",
        "    text = text[1:-1]\n",
        "    arr = np.fromstring(text,sep=',', dtype=int)\n",
        "    return arr\n",
        "    # return torch.LongTensor(arr)\n",
        "\n",
        "  def get_next_batch(self):\n",
        "    batch = []\n",
        "    while len(batch) < self.batch_size:\n",
        "      batch.append(self.get_next_seq())\n",
        "      # batch.append(torch.LongTensor(self.get_next_seq()))\n",
        "    if len(batch) > self.batch_size:\n",
        "      batch = batch[0:self.batch_size]\n",
        "    # return torch.LongTensor(np.array(batch))\n",
        "    return np.array(batch)\n"
      ],
      "metadata": {
        "id": "xrx7jPyQq-4i"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title StoryDataLoader\n",
        "class StoryDataLoader():\n",
        " \n",
        "  def __init__(self, df_filename, tokenizer, batch_size, df_file_len, num_seqs_per_story, context_len, load_stories_at_a_time):\n",
        "    self.df_filename = df_filename\n",
        "    self.tokenizer = tokenizer\n",
        "    self.batch_size = batch_size\n",
        "    self.total_df_len = df_file_len\n",
        "    self.remaining_df_items = df_file_len\n",
        "    self.num_seqs_per_story = num_seqs_per_story\n",
        "    self.context_len = context_len\n",
        "    self.all_subsequences_from_story = []\n",
        "    self.load_stories_at_a_time = load_stories_at_a_time\n",
        "    self.DF_BUFFER = 100 + self.load_stories_at_a_time\n",
        "    self.df = self.init_df()\n",
        "    self.story_buffer = []\n",
        "\n",
        "\n",
        "  def init_df(self):\n",
        "     self.remaining_df_items = self.total_df_len\n",
        "     df = pd.read_csv(self.df_filename, \n",
        "                 index_col=0,\n",
        "                 compression={'method': 'gzip', 'compresslevel': 2}, \n",
        "                 chunksize=self.load_stories_at_a_time, \n",
        "                 iterator=True)\n",
        "     return df\n",
        "\n",
        "  def _get_random_subsequence(self, seq, subsequence_len):\n",
        "    start_index = random.randint(0, len(seq) - subsequence_len)\n",
        "    return seq[start_index:start_index + subsequence_len]\n",
        "\n",
        "  def has_next_batch(self):\n",
        "    return self.remaining_df_items > self.DF_BUFFER\n",
        "\n",
        "  def get_next_story(self):\n",
        "    if len(self.story_buffer) <= 0:\n",
        "      chunk = self.df.get_chunk()\n",
        "      for item in chunk['full_text']:\n",
        "        self.story_buffer.append(str(item))\n",
        "    return self.story_buffer.pop()\n",
        "  \n",
        "  def get_tokens(self):\n",
        "    # tokens = []\n",
        "    while True:\n",
        "      try:\n",
        "        story = self.df.get_next_story()\n",
        "        self.remaining_df_items -= 1 #self.load_stories_at_a_time\n",
        "      except:\n",
        "        self.remaining_df_items = 0\n",
        "        raise Exception(\"Next batch\")\n",
        "          \n",
        "      text = story\n",
        "      tokens = tokenizer.encode(text)\n",
        "      # tokens = torch.Tensor(tokens).to(torch.long).to(device)\n",
        "      tokens = [tokenizer.bos_token_id] + tokens + [tokenizer.eos_token_id] + self.context_len * [tokenizer.pad_token_id]\n",
        "      # print(len(tokens))\n",
        "      if len(tokens) <= self.context_len:\n",
        "        print(\"Story too short, retrying now\")\n",
        "        continue\n",
        "      else:\n",
        "        return tokens\n",
        "\n",
        "  def get_next_batch(self):\n",
        "    batch = []\n",
        "    while len(batch) < self.batch_size:\n",
        "      if len(self.all_subsequences_from_story) == 0:\n",
        "        tokens = self.get_tokens()\n",
        "        # tokens = tokens + [tokenizer.pad_token_id] * self.context_len\n",
        "        # print(tokens)\n",
        "\n",
        "        for _ in range(self.num_seqs_per_story):\n",
        "          subseq = self._get_random_subsequence(tokens, self.context_len-1)\n",
        "          subseq = [tokenizer.cls_token_id] + subseq\n",
        "          self.all_subsequences_from_story.append(subseq)\n",
        "      subseq = self.all_subsequences_from_story.pop()\n",
        "      batch.append(subseq)\n",
        "    if len(batch) > self.batch_size:\n",
        "      batch = batch[0:self.batch_size]\n",
        "    return batch\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QPKiEBC5cOpQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title SingleStoryDataLoader\n",
        "class SingleStoryDataLoader():\n",
        " \n",
        "  def __init__(self, text_filename, tokenizer, batch_size, context_len):\n",
        "    self.text_filename = text_filename\n",
        "    self.tokenizer = tokenizer\n",
        "    self.batch_size = batch_size\n",
        "    self.context_len = context_len\n",
        "    self.all_text_tokens = self.get_text_tokens()\n",
        "\n",
        "\n",
        "  def get_text_tokens(self):\n",
        "    with open(self.text_filename) as text_file:\n",
        "      uncleaned_text = text_file.read()\n",
        "    tokens = tokenizer.encode(uncleaned_text)\n",
        "    tokens = [tokenizer.bos_token_id] + tokens + [tokenizer.eos_token_id] + self.context_len * [tokenizer.pad_token_id]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "  def _get_random_subsequence(self, seq, subsequence_len):\n",
        "    start_index = random.randint(0, len(seq) - subsequence_len)\n",
        "    return seq[start_index:start_index + subsequence_len]\n",
        "\n",
        "  def has_next_batch(self):\n",
        "    return True\n",
        "\n",
        "  def get_next_batch(self):\n",
        "    batch = []\n",
        "    while len(batch) < self.batch_size:\n",
        "      subseq = self._get_random_subsequence(self.all_text_tokens, self.context_len-1)\n",
        "      subseq = [tokenizer.cls_token_id] + subseq\n",
        "      batch.append(subseq)\n",
        "    if len(batch) > self.batch_size:\n",
        "      batch = batch[0:self.batch_size]\n",
        "    return batch\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "orIDFqxcWCrd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title SingleStoryBertAssistDataLoader\n",
        "class SingleStoryBertAssistDataLoader():\n",
        " \n",
        "  def __init__(self, text_filename, tokenizer, batch_size, context_len, bert_model, mask_percentage=.15):\n",
        "    self.text_filename = text_filename\n",
        "    self.tokenizer = tokenizer\n",
        "    self.batch_size = batch_size\n",
        "    self.context_len = context_len\n",
        "    self.all_text_tokens = self.get_text_tokens()\n",
        "    self.bert_model = bert_model\n",
        "    self.mask_percentage = mask_percentage\n",
        "    self.mask_token_id = self.tokenizer.mask_token_id # bert_model.tokenizer.mask_token_id\n",
        "\n",
        "  def get_text_tokens(self):\n",
        "    with open(self.text_filename) as text_file:\n",
        "      uncleaned_text = text_file.read()\n",
        "    tokens = self.tokenizer.encode(uncleaned_text)\n",
        "    tokens = tokens + self.context_len * [self.tokenizer.pad_token_id]\n",
        "    return tokens\n",
        "\n",
        "  def _get_random_subsequence(self, seq, subsequence_len):\n",
        "    start_index = random.randint(0, len(seq) - subsequence_len)\n",
        "    return seq[start_index:start_index + subsequence_len]\n",
        "\n",
        "  def _augment_subsequence(self, seq):\n",
        "    rand = torch.rand(len(seq))\n",
        "    mask_selection = rand < self.mask_percentage\n",
        "    mask_selection = torch.flatten(mask_selection.nonzero()).tolist()\n",
        "    # print(mask_selection)\n",
        "    # mask_selection = [2,4,6,8]\n",
        "    # old_ids = []\n",
        "    # print(\"Pre-change text: \",self.tokenizer.decode(seq) )\n",
        "    for idx in mask_selection:\n",
        "      # old_ids.append(seq[idx])\n",
        "      seq[idx] = self.mask_token_id\n",
        "    # text = self.tokenizer.decode(seq)\n",
        "    processed_seq = torch.Tensor([seq]).to(torch.long).to(device)\n",
        "    # print(seq)\n",
        "    # print(\"Conversion text: \",self.tokenizer.decode(seq) )\n",
        "    new_words = self.bert_model(processed_seq)\n",
        "    # print(unmasked)\n",
        "    new_words = new_words['logits'][0]\n",
        "    # print(new_words)\n",
        "    # print(seq)\n",
        "    for idx, new_word in enumerate(new_words):\n",
        "      if idx not in mask_selection:\n",
        "        continue\n",
        "      else:\n",
        "        new_word_id = torch.argmax(new_word)\n",
        "        # print(seq, new_word_id)\n",
        "        seq[idx] = int(new_word_id)\n",
        "    return seq\n",
        "\n",
        "  def has_next_batch(self):\n",
        "    return True\n",
        "\n",
        "  def get_next_batch(self):\n",
        "    batch = []\n",
        "    while len(batch) < self.batch_size:\n",
        "      subseq = self._get_random_subsequence(self.all_text_tokens, self.context_len-1)\n",
        "      subseq = self._augment_subsequence(subseq)\n",
        "      subseq = [tokenizer.cls_token_id] + subseq + 50 * [tokenizer.pad_token_id]\n",
        "      subseq = subseq[0:self.context_len]\n",
        "      batch.append(subseq)\n",
        "    if len(batch) > self.batch_size:\n",
        "      batch = batch[0:self.batch_size]\n",
        "    return batch"
      ],
      "metadata": {
        "cellView": "form",
        "id": "z9i-2DkRcE48"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = BertChunkDataLoader(\\\n",
        "                                       df_filename=bibliotik_train_tokenized,\n",
        "                                       batch_size=config[\"batch_size\"],\n",
        "                                       df_file_len=BIBLIOTIK_TRAIN_TOKENIZED_LEN,\n",
        "                                       load_chunk_size=config[\"load_chunk_size\"],\n",
        "                                      )\n",
        "val_dataloader = BertChunkDataLoader(\\\n",
        "                                       df_filename=bibliotik_val_tokenized,\n",
        "                                       batch_size=config[\"val_batch_size\"],\n",
        "                                       df_file_len=BIBLIOTIK_VAL_TOKENIZED_LEN,\n",
        "                                       load_chunk_size=config[\"load_chunk_size\"],\n",
        "                                      )"
      ],
      "metadata": {
        "id": "5PJWiLw5sc3P"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if test_parts:\n",
        "  for i in range(4):\n",
        "    batch = train_dataloader.get_next_batch()\n",
        "    print(len(batch), type(batch[0][0:10]), batch[0][0:10])\n",
        "  for i in range(4):\n",
        "    batch = val_dataloader.get_next_batch()\n",
        "    print(len(batch), batch[0][0:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbIHVczy2uGL",
        "outputId": "18a7f7de-7390-41b7-f890-ca12b7b1c895"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 <class 'numpy.ndarray'> [3011 1228  395  199  199   41 3043   14  371  989]\n",
            "2 <class 'numpy.ndarray'> [  824    12 15738   291   821 38989   432   433    13    19]\n",
            "2 <class 'numpy.ndarray'> [ 9424 12990  1983  7146   439   199 11586  4151    12  2339]\n",
            "2 <class 'numpy.ndarray'> [ 3492   199   199    22   287  1619 35792   335  1645 19144]\n",
            "2 [24407    12  1100   288   261  1332  5123 25650   376   658]\n",
            "2 [  14 1178  744   12  348  894 4429  261 9866 3279]\n",
            "2 [  434 11962 10251   866   291   261  8146    14   199   199]\n",
            "2 [  854   309   261  5903   338   261 47735 15506   348  9095]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup masking"
      ],
      "metadata": {
        "id": "6ohMrCutTbTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_batch_do_nothing(batch):\n",
        "  masked_batch = []\n",
        "  for row in batch:\n",
        "    # print(row)\n",
        "    masked_row = row.copy()\n",
        "    # masked_row[2] = 9\n",
        "    # masked_batch = np.concatenate((masked_batch,masked_row))\n",
        "    masked_batch.append(masked_row)\n",
        "  masked_batch = np.array(masked_batch)\n",
        "  return format_batches(masked_batch, batch)\n",
        "\n",
        "def format_batches(masked_batch, batch):\n",
        "  masked_batch = torch.LongTensor(masked_batch).to(config[\"device\"])\n",
        "  batch = torch.LongTensor(batch).to(config[\"device\"])\n",
        "  return masked_batch, batch"
      ],
      "metadata": {
        "id": "kElPZ-mvTe8G"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if test_parts:\n",
        "  batch = train_dataloader.get_next_batch()\n",
        "  masked_batch, batch = mask_batch_do_nothing(batch)\n",
        "  print(np.shape(masked_batch))\n",
        "  print(masked_batch[0][0:20])\n",
        "  print(batch[0][0:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhx4vcPAT6Wh",
        "outputId": "827be0dd-8472-4bb4-93c2-c4558a5a65c7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 256])\n",
            "tensor([ 5126,    12,  2013,    12,   261,  4440, 13137, 27544,   443, 11707,\n",
            "          381,  3407,   288, 17283,   961,   381,  7414,    14,   199,   199],\n",
            "       device='cuda:0')\n",
            "tensor([ 5126,    12,  2013,    12,   261,  4440, 13137, 27544,   443, 11707,\n",
            "          381,  3407,   288, 17283,   961,   381,  7414,    14,   199,   199],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set up the model"
      ],
      "metadata": {
        "id": "-RMatIDBPkbt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "h4zywtNaNE5k"
      },
      "outputs": [],
      "source": [
        "model = bert.BertModel(d_model = config[\"d_model\"], \n",
        "                        vocab_length = config[\"vocab_length\"], \n",
        "                        sequence_length = config['context_len'],\n",
        "                        num_layers = config[\"num_layers\"], \n",
        "                        feed_forward_dimensions = config[\"feed_forward_dimensions\"], \n",
        "                        attention_heads = config[\"attention_heads\"],\n",
        "                        attention_qkv_dims =  config[\"attention_qkv_dims\"], \n",
        "                        dropout = config[\"dropout\"], \n",
        "                        pad_idx = tokenizer.pad_token_id, \n",
        "                        device = config[\"device\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5JHO_adNE5l",
        "outputId": "466ec03b-efe6-45bc-955e-4fa202dc29a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 10, 52005])\n"
          ]
        }
      ],
      "source": [
        "if test_parts:\n",
        "    inp = torch.LongTensor([[0,1,2,3,4,5,6,7,8,29],[1,3,4,5,6,8,0,22,1,1]])\n",
        "    out = model(inp)\n",
        "    print(out.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "DLzZemWbNE5l"
      },
      "outputs": [],
      "source": [
        "model = model.to(config[\"device\"])\n",
        "model = model.cuda(0)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "eVXP07moNE5m",
        "outputId": "6ddab8d8-0ab3-4da9-ff28-718b8a11f169"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beginning Epoch 0\n",
            "cuda:0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-98ac7aa078e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# print(model.type())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mlogits_lm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_lm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss_lm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_lm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# for masked LM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/bert_git/bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mattn_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_attn_pad_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/bert_git/bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (seq_len,) -> (batch_size, seq_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtok_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2208\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)"
          ]
        }
      ],
      "source": [
        "# batch = torch.LongTensor(train_dataloader.get_next_batch())\n",
        "# print(batch.size())\n",
        "# input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
        "\n",
        "\n",
        "for epoch in range(config[\"epochs\"]):\n",
        "    model.train()\n",
        "    print(f\"Beginning Epoch {epoch}\")\n",
        "    for i in range(config[\"batches_per_epoch\"]):\n",
        "      with autocast('cuda'):\n",
        "        macked_batch, batch = mask_batch_do_nothing(train_dataloader.get_next_batch())\n",
        "        optimizer.zero_grad()\n",
        "        print(batch.device)\n",
        "        # print(model.type())\n",
        "        logits_lm = model.to(config[\"device\"])(batch.to(config[\"device\"]))\n",
        "        print(logits_lm.size(), batch.size())\n",
        "        loss_lm = criterion(logits_lm.transpose(1, 2), batch) # for masked LM\n",
        "        loss_lm = (loss_lm.float()).mean()\n",
        "        # loss_clsf = criterion(logits_clsf, isNext) # for sentence classification\n",
        "        wandb.log({\"Loss\": loss_lm})\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss_lm))\n",
        "        loss_lm.backward()\n",
        "        optimizer.step()\n",
        "    for j in range(config[\"batches_per_val\"]):\n",
        "      macked_batch, batch = mask_batch_do_nothing(val_dataloader.get_next_batch())\n",
        "      optimizer.zero_grad()\n",
        "      logits_lm = model(batch)\n",
        "      loss_lm = criterion(logits_lm.transpose(1, 2), batch) # for masked LM\n",
        "      loss_lm = (loss_lm.float()).mean()\n",
        "      wandb.log({\"Validation Loss\": loss_lm})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-23iqZnNE5n"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e5cdae407986fbcf9f40eb4f2caf8136385e94546bed8444298080b1cba2358b"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}