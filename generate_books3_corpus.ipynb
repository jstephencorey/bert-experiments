{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Generating the books3 corpus of 256-length token chunks for BERT training"
      ],
      "metadata": {
        "id": "lcgzBL2gQ9Yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --quiet"
      ],
      "metadata": {
        "id": "eFMp3zsnRlbQ",
        "outputId": "e3a57acb-9979-4cb5-e414-750ab2f2fe7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 5.8 MB 7.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 60.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 53.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 6.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 62.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 174 kB 53.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 173 kB 53.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 168 kB 76.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 168 kB 60.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 166 kB 59.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 166 kB 51.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 162 kB 60.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 162 kB 52.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 158 kB 75.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 53.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 62.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 60.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 61.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 74.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 56.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 76.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 156 kB 28.6 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qs1uwaH3Pw8d",
        "outputId": "7153d69f-5944-4e5a-cf27-74c23b92ca47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import random\n",
        "from transformers import set_seed, AutoTokenizer\n",
        "\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = '/content/drive/MyDrive/AI_Data/'\n",
        "bibliotik_train = f'{DATA_DIR}bibliotik_corpus/biblitik_22500_full.gz'\n",
        "bibliotik_val = f'{DATA_DIR}bibliotik_corpus/biblitik_7500_val.gz'\n",
        "\n",
        "MODEL_DIR = '/content/drive/MyDrive/AI_Models/storygpt/'\n",
        "TOKENIZER_DIR = f'{MODEL_DIR}storygpt2tokenizer_ft2/'"
      ],
      "metadata": {
        "id": "QF-cGvnQQsSM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Tokenizer"
      ],
      "metadata": {
        "id": "fTDwrFLYS2qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BOS_TOKEN = '<BOS>'\n",
        "EOS_TOKEN = '<EOS>'\n",
        "PAD_TOKEN = '<PAD>'\n",
        "CLS_TOKEN = '[CLS]'\n",
        "MASK_TOKEN = '[MASK]'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)\n",
        "special_tokens_dict = {'cls_token':CLS_TOKEN, 'bos_token': BOS_TOKEN, 'eos_token': EOS_TOKEN, 'pad_token': PAD_TOKEN, 'mask_token': MASK_TOKEN}\n",
        "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "VOCAB_SIZE = len(tokenizer)\n",
        "print(VOCAB_SIZE)"
      ],
      "metadata": {
        "id": "eHed5GHUSyIs",
        "outputId": "9eec899f-8d01-46b7-fe22-6c08d0b40806",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "52005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Data from Google Drive"
      ],
      "metadata": {
        "id": "FvQ59N05UHQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filename = bibliotik_train\n",
        "df = pd.read_csv(df_filename, \n",
        "            index_col=0,\n",
        "            compression={'method': 'gzip', 'compresslevel': 2}, \n",
        "            chunksize=1, \n",
        "            iterator=True)"
      ],
      "metadata": {
        "id": "yszy2eltUMbQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Get n random token sequences from each story"
      ],
      "metadata": {
        "id": "lv132XTKUc75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TOK_SEQS_PER_STORY = 100\n",
        "SEQ_LEN = 256\n",
        "NUM_PAD_TOKENS = 128\n",
        "all_token_seqs = []"
      ],
      "metadata": {
        "id": "f0jV4hq-UZOM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_random_subsequence(seq, subsequence_len):\n",
        "    start_index = random.randint(0, len(seq) - subsequence_len)\n",
        "    return seq[start_index:start_index + subsequence_len]"
      ],
      "metadata": {
        "id": "6T4UnpSLVwXo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "story = df.get_chunk()['full_text'][0]\n",
        "all_tokens = tokenizer.encode(story)\n",
        "all_tokens = [tokenizer.bos_token_id] + all_tokens + [tokenizer.eos_token_id] + NUM_PAD_TOKENS * [tokenizer.pad_token_id]\n",
        "\n",
        "for _ in range(TOK_SEQS_PER_STORY):\n",
        "  token_seq = get_random_subsequence(all_tokens, len(all_tokens))\n",
        "  all_token_seqs.append(token_seq)\n",
        "print(len(all_token_seqs))"
      ],
      "metadata": {
        "id": "Pk3ZaE8WU59A",
        "outputId": "a868a905-6b97-496f-cb18-cf4073503b68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DfDataLoader():\n",
        " \n",
        "  def __init__(self, df_filename, tokenizer, batch_size, df_file_len, num_seqs_per_story, context_len):\n",
        "    self.df_filename = df_filename\n",
        "    self.tokenizer = tokenizer\n",
        "    self.batch_size = batch_size\n",
        "    self.total_df_len = df_file_len\n",
        "    self.remaining_df_items = df_file_len\n",
        "    self.num_seqs_per_story = num_seqs_per_story\n",
        "    self.context_len = context_len\n",
        "    self.DF_BUFFER = 100\n",
        "    self.all_subsequences_from_story = []\n",
        "    self.df = self.init_df()\n",
        "\n",
        "\n",
        "  def init_df(self):\n",
        "     self.remaining_df_items = self.total_df_len\n",
        "     df = pd.read_csv(self.df_filename, \n",
        "                 index_col=0,\n",
        "                 compression={'method': 'gzip', 'compresslevel': 2}, \n",
        "                 chunksize=1, \n",
        "                 iterator=True)\n",
        "     return df\n",
        "\n",
        "  def _get_random_subsequence(self, seq, subsequence_len):\n",
        "    start_index = random.randint(0, len(seq) - subsequence_len)\n",
        "    return seq[start_index:start_index + subsequence_len]\n",
        "\n",
        "  def has_next_batch(self):\n",
        "    return self.remaining_df_items > self.DF_BUFFER\n",
        "  \n",
        "  def get_tokens(self):\n",
        "    # tokens = []\n",
        "    while True:\n",
        "      try:\n",
        "        chunk = self.df.get_chunk()\n",
        "        self.remaining_df_items -= 1\n",
        "      except:\n",
        "        self.remaining_df_items = 0\n",
        "        raise Exception(\"Next batch\")\n",
        "          \n",
        "      text = str(chunk['full_text'][0])\n",
        "      tokens = tokenizer.encode(text)\n",
        "      tokens = [tokenizer.bos_token_id] + tokens + [tokenizer.eos_token_id] + self.context_len * [tokenizer.pad_token_id]\n",
        "      # print(len(tokens))\n",
        "      if len(tokens) <= self.context_len:\n",
        "        print(\"Story too short, retrying now\")\n",
        "        continue\n",
        "      else:\n",
        "        return tokens\n",
        "\n",
        "  def get_next_batch(self):\n",
        "    batch = []\n",
        "    while len(batch) < self.batch_size:\n",
        "      if len(self.all_subsequences_from_story) == 0:\n",
        "        tokens = self.get_tokens()\n",
        "        # tokens = tokens + [tokenizer.pad_token_id] * self.context_len\n",
        "\n",
        "        for _ in range(self.num_seqs_per_story):\n",
        "          subseq = self._get_random_subsequence(tokens, self.context_len-1)\n",
        "          subseq = [tokenizer.cls_token_id] + subseq\n",
        "          self.all_subsequences_from_story.append(subseq)\n",
        "      subseq = self.all_subsequences_from_story.pop()\n",
        "      batch.append(subseq)\n",
        "    if len(batch) > self.batch_size:\n",
        "      batch = batch[0:self.batch_size]\n",
        "    return batch\n",
        "\n",
        "\n",
        "train_dataloader = SingleStoryBertAssistDataLoader(\\\n",
        "                                   text_filename=atlas_shrugged_filename,\n",
        "                                   tokenizer=tokenizer,\n",
        "                                   batch_size = config['batch_size'],\n",
        "                                   context_len=config['context_len'],\n",
        "                                   bert_model = bert_model,\n",
        "                                   mask_percentage = config['bert_mask_percentage']\n",
        "                                    )"
      ],
      "metadata": {
        "id": "IlITDgaES43L"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e5cdae407986fbcf9f40eb4f2caf8136385e94546bed8444298080b1cba2358b"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}