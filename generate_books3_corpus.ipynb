{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Generating the books3 corpus of 256-length token chunks for BERT training"
      ],
      "metadata": {
        "id": "lcgzBL2gQ9Yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --quiet"
      ],
      "metadata": {
        "id": "eFMp3zsnRlbQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qs1uwaH3Pw8d",
        "outputId": "2152abad-61d2-4685-bf86-f1ee26f46698"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import random\n",
        "from transformers import set_seed, AutoTokenizer\n",
        "\n",
        "drive.mount('/content/drive', force_remount=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = '/content/drive/MyDrive/AI_Data/'\n",
        "bibliotik_train = f'{DATA_DIR}bibliotik_corpus/biblitik_22500_full.gz'\n",
        "bibliotik_val = f'{DATA_DIR}bibliotik_corpus/biblitik_7500_val.gz'\n",
        "\n",
        "MODEL_DIR = '/content/drive/MyDrive/AI_Models/storygpt/'\n",
        "TOKENIZER_DIR = f'{MODEL_DIR}storygpt2tokenizer_ft2/'"
      ],
      "metadata": {
        "id": "QF-cGvnQQsSM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Tokenizer"
      ],
      "metadata": {
        "id": "fTDwrFLYS2qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BOS_TOKEN = '<BOS>'\n",
        "EOS_TOKEN = '<EOS>'\n",
        "PAD_TOKEN = '<PAD>'\n",
        "CLS_TOKEN = '[CLS]'\n",
        "MASK_TOKEN = '[MASK]'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)\n",
        "special_tokens_dict = {'cls_token':CLS_TOKEN, 'bos_token': BOS_TOKEN, 'eos_token': EOS_TOKEN, 'pad_token': PAD_TOKEN, 'mask_token': MASK_TOKEN}\n",
        "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "VOCAB_SIZE = len(tokenizer)\n",
        "print(VOCAB_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHed5GHUSyIs",
        "outputId": "daffbce1-6766-4ac6-fedb-9f6d0260e09f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "52005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Get n random token sequences from each story"
      ],
      "metadata": {
        "id": "lv132XTKUc75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load Data from Google Drive"
      ],
      "metadata": {
        "id": "FvQ59N05UHQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TOK_SEQS_PER_STORY = 100\n",
        "SEQ_LEN = 256\n",
        "NUM_PAD_TOKENS = 128\n",
        "READ_CHUNKSIZE = 100\n",
        "FILTER_OUT_SHORTER_THAN = 1200\n",
        "all_token_seqs = []\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "f0jV4hq-UZOM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filename = bibliotik_train\n",
        "df = pd.read_csv(df_filename, \n",
        "            index_col=0,\n",
        "            compression={'method': 'gzip', 'compresslevel': 2}, \n",
        "            chunksize=READ_CHUNKSIZE, \n",
        "            iterator=True)"
      ],
      "metadata": {
        "id": "yszy2eltUMbQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_random_subsequence(seq, subsequence_len):\n",
        "    start_index = random.randint(0, len(seq) - subsequence_len)\n",
        "    return seq[start_index:start_index + subsequence_len]"
      ],
      "metadata": {
        "id": "6T4UnpSLVwXo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokens_from_corpus(df, df_len):\n",
        "  all_token_seqs = []\n",
        "  _avg_time = 0\n",
        "  for i in range(df_len // READ_CHUNKSIZE):\n",
        "    print(f\"processings chunk {i}\")\n",
        "    stories = df.get_chunk()['full_text'].to_list()\n",
        "    # if len(story) <= 100:\n",
        "    #   \n",
        "    # stories = stories.\n",
        "    # print(stories)\n",
        "    filtered_stories = []\n",
        "    for story in stories:\n",
        "      # length = min(len(story) //10, 100)\n",
        "      # print(\"STORY_LEN: \",len(story))\n",
        "      _total_story_len = 0\n",
        "      if type(story) != type('hello world'):\n",
        "        print(\"Unexpected value: \", story)\n",
        "        continue\n",
        "      elif len(story) <= FILTER_OUT_SHORTER_THAN: # Roughly the len of 256 tokens?\n",
        "        print(\"skipping small story of len \", len(story))\n",
        "        continue\n",
        "      else:\n",
        "        filtered_stories.append(story)\n",
        "        _total_story_len += len(story)\n",
        "    # print(len(filtered_stories))\n",
        "    if len(filtered_stories) == 0:\n",
        "      continue\n",
        "    start_time = time.time()\n",
        "    tokenized_stories = tokenizer.batch_encode_plus(filtered_stories)\n",
        "    tokenized_stories = tokenized_stories['input_ids']\n",
        "    _total_story_len = max(1, _total_story_len)\n",
        "    print(((time.time() - start_time) / _total_story_len)*100000)\n",
        "    for tokenized_story in tokenized_stories:\n",
        "      # print()\n",
        "      story_tokens = [tokenizer.bos_token_id] + tokenized_story + [tokenizer.eos_token_id] + NUM_PAD_TOKENS * [tokenizer.pad_token_id]\n",
        "\n",
        "      for _ in range(TOK_SEQS_PER_STORY):\n",
        "        token_seq = get_random_subsequence(story_tokens, SEQ_LEN)\n",
        "        all_token_seqs.append([token_seq])\n",
        "      \n",
        "    # if i >= 50:\n",
        "    #   break\n",
        "  return all_token_seqs\n",
        "\n",
        "all_token_seqs = get_tokens_from_corpus(df, 22500)\n",
        "print(len(all_token_seqs))\n",
        "# Chunksize = 100 : 5m 50 s to do first 1000\n",
        "# Chunksize = 1 : 3m 19 s to do first 453\n",
        "# Chunksize = 10 : 3m 5 s to do first 500"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk3ZaE8WU59A",
        "outputId": "2ce9ee8b-e4a2-46a3-d3c4-f37be3bf7c22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processings chunk 0\n",
            "12.102200015629673\n",
            "processings chunk 1\n",
            "3.4839452919707017\n",
            "processings chunk 2\n",
            "4.8139285614823715\n",
            "processings chunk 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(all_token_seqs[0:10])\n",
        "random.shuffle(all_token_seqs)\n",
        "seqs_df = pd.DataFrame(all_token_seqs, columns=['token_seqs'])\n",
        "seqs_df.head()"
      ],
      "metadata": {
        "id": "Z2ZyqB4di92A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seqs_df.to_csv(f'{DATA_DIR}bibliotik_corpus/biblitik_22500_256_tokenized.gz', compression={'method': 'gzip', 'compresslevel': 2},)"
      ],
      "metadata": {
        "id": "k1KNE8srkURh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "time.sleep(100)\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "C09Js-Vr4rW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ";;"
      ],
      "metadata": {
        "id": "IlITDgaES43L"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e5cdae407986fbcf9f40eb4f2caf8136385e94546bed8444298080b1cba2358b"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}